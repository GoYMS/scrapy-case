#爬虫框架
   -scrapy
   -pyspider
   
   -一个爬虫所需的组件
          爬虫调度器
          url管理器
          html下载器
          html解析器
          数据存储器

#scrapy框架学习
    -框架
    -异步网络框架Twisted（默认自带多线程）
    -提供各种接口以及中间件

#scrapy长什么样子
     -Spider（爬虫）
          - 1.初次发起我们的爬虫请求
          - 2.解析response得到的数据，若是url地址将url传递给调度器进行循环爬取，若是数据传递给item pipeline
     -Scheduler（调度器）
          - 负责接受引擎发送过来的requests请求，在此处进行队列的整合
     -downloader（下载器）
          -主要负责从互联网进行网页内容的请求
     -Item Pipline（数据存储）
          -主要负责spider中得到数据（item）,进行数据的处理与保存
     -Scrapy Engine（引擎）
           - 负责spider，Itenpine，scheduler，download之间的协调与通讯/数据传递
     -Download Middlewares：
           - 下载中间件，主要进行仔仔功能的扩展
     -Spider Middlewares:
           -主要进行扩展spider功能/扩展与引擎之间的通信功能

#scrapy框架的安装
     --搭建虚拟环境
         -查看所有的虚拟环境
            -conda env list
         -创建虚拟环境
            -conda create -n scrapy_env python=3.6
         -创建完成激活虚拟环境
            -source activate scrapy_env
         -安装scrapy框架
            -pip install scrapy



#创建一个scrapy项目
      -首先进入虚拟环境
         source activate scrapy_env
      -选择在哪个下边创建就转换到相应的环境中
         cd   路径
      -创建项目
          scrapy startproject 项目名称
      #创建爬虫实例
          scrapy genspider 爬虫名称  域名（例如：baidu.com）


      #运行
        1.-首先进去相应虚拟环境，然后进去项目路径中   执行  scrapy crawl 爬虫名称

        2. 或者创建一个文件进行



#Scrapy Shell
       -交互终端
       -python开发环境下
       -ipython
#启动Scrapy Shell
       -scrapy shell 'www.baidu.com'
       -response.body.decode('utf-8')
#Scrapy 提供的选择器
       -基本选择器
           -xpath() :传入xpath表达式，得到selector list
           -extract() :序列化该节点末unicode字符串列表
           -css() : 传入css表达式，返回selector list ，语法规则同BeautifulSoup4一样
           -re() : 传入正则表达式进行规则匹配
#scrapy日志等级
     -DEBUG :调试信息
     -INFO ：一般信息
     -WARNING：警告信息
     -ERROR :一般错误
     -CRITICAL：严重错误

#scrapy 日志设置
   -LOG_ENABLED:默认Ture：启用日志
   -LOG_ENCODING :编码类型，默认是utf-8
   -LOG_FILE:日志的输出文件，默认当前路径下，可改
   -LOG_LEVEL:默认DEBUG

    
  